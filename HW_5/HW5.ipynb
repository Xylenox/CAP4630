{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW5",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNYv3ziNQrTtVAyF4WicI4z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Xylenox/CAP4630/blob/master/HW_5/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TCFO2mewZ3v",
        "colab_type": "text"
      },
      "source": [
        "# General Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7EwAAwc9xI1X",
        "colab_type": "text"
      },
      "source": [
        "## Aritifical Intelligence\n",
        "Artificial Intelligence is \"the science and engineering of making intelligent machines.\" It's basically making machines that can make decisions without human input.\n",
        "\n",
        "# Machine Learning\n",
        "Machine learning is the technique of letting machines take in information and use it to learn how to do things better. Instead of always doing the same thing on the same input, the machine can learn how to do the task better and return a better result.\n",
        "\n",
        "# Deep Learning\n",
        "Deep learning is a subst of machine learning that relies on neural networks. They are inspired by biological neurons in animal brains. They aren't programmed from a specific task, and learn from a set of examples to do some task."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UHYWqmcJGUe3"
      },
      "source": [
        "# Basic Concepts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUm1I5xZAkT7",
        "colab_type": "text"
      },
      "source": [
        "## Linear Regression\n",
        "Linear regression is a type of regression that predicts the output value as a linear combination of the input features plus a bias.\n",
        "\n",
        "A linear regression model has a set of weights, $w=(w_1, \\ldots, w_n)$, and a bias $b$. If the input is $x=(x_1, \\ldots, x_n)$, then the output will be $w_1*x_1 + \\ldots + w_2*x_2 + b$.\n",
        "\n",
        "## Logistic Regression\n",
        "Logistic regression is a type of regression that predicts the probability that a set of input features is one of two different classes. Logistic regression is very similar to linear regression in that it as a set of weights and a bias. The difference is that instead of the output being $w_1*x_1 + \\ldots + w_2*x_2 + b$, the out put is $\\sigma(w_1*x_1 + \\ldots + w_2*x_2 + b)$. $\\sigma(x) = \\frac{1}{1+e^{-x}}$, also known as the sigmoid function.\n",
        "\n",
        "## Gradient\n",
        "The gradient of a function at a specific point is the vector in which the change of the function in that direction is maximal. In other words, give a function $f(x)$, where $x=(x_1, \\ldots, x_n)$, the gradient of $f$ is equal to $(\\frac{\\partial f}{\\partial x_1}, \\ldots, \\frac{\\partial f}{\\partial x_n})$.\n",
        "\n",
        "## Gradient Descent\n",
        "Gradient Descent is a technique used to find a local minimum of a function. The input is initialized to arbitrary values. Basically, gradient descent repeatedly moves the input a small amount, called the learning rate, in the direction of the gradient.\n",
        "\n",
        "## Loss Function\n",
        "The loss function of a model is how close to the correct answers a model is on a set of inputs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3Y8eNlxXEIU",
        "colab_type": "text"
      },
      "source": [
        "# Building a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjZpjSklc8Mz",
        "colab_type": "text"
      },
      "source": [
        "A convnet consists of two parts, the convolutional base and the classifier.\n",
        "\n",
        "## Convolutional Base\n",
        "The conv base consists of mutiple groups of convolutional filters and max pooling layers. The first convolution layer describes specific details, such as edges of a picture, while later convolutional layers describe more abstract concepts, such as an ear or an eye.\n",
        "\n",
        "Convolutional bases can generally be reused for different projects, or slightly modified between them.\n",
        "\n",
        "## Classifier\n",
        "The classifier consists of a series of dense layers. It takes the output from the convolutional base and turns it into the probability of some set of classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cjh4xpKl6-zS",
        "colab_type": "text"
      },
      "source": [
        "# Compiling a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_42PNJZs9TkT",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer\n",
        "Optimizers are ways of updating a models parameters in response to the loss function. Gradient descent is an example of an optimizer in that it changes the weights in the direction of the gradient of the loss function.\n",
        "\n",
        "## Learning Rate\n",
        "The learning rate is a parameter passed into an optimizer that controls how fast the model learns. If the model learns too fast, it could overshoot the minimum and end up with a suboptimal result."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7GKChKiVurQ",
        "colab_type": "text"
      },
      "source": [
        "# Training a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSUsFd_UVwnU",
        "colab_type": "text"
      },
      "source": [
        "Two problems that can occur during training are overfitting and underfitting.\n",
        "\n",
        "## Overfitting\n",
        "Overfitting is when your model is too complex for the problem you are working on. The model would be very accurate on the training data, but not very accurate on the validation data. One way to deal with overfitting is to change the training data a bit, such as rotating or scaling images. Another way is to add a dropout layer, which removes a few nodes in the network each time it trains. Also, making sure to get redundant input features is important.\n",
        "\n",
        "## Underfitting\n",
        "Underfitting is when the model is not complex enough to fit the data. The model either doesn't have very good accuracy on the train data or it takes a long time to train. The way to fix this is to just increase the complexity of the model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7tohSWxloWf",
        "colab_type": "text"
      },
      "source": [
        "# Finetuning a Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWTEu0GXlqwE",
        "colab_type": "text"
      },
      "source": [
        "Instead of creating a conv base from scratch, it might be worthwile reusing a pre-trained base, such as VGG-16. Since these are trained on a large database of images, they are very generalized. Then, you can attach your own classifier to it. The conv base should be locked from training, or else it might change dramatically during the initial training.\n",
        "\n",
        "After the initial training, when the classifier is more adapted to the base, a few layers near the bottom of the base can be unfrozen to finetune them. This generally leads to better results than just training the classifier, but can also lead to more overfitting."
      ]
    }
  ]
}